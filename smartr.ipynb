{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This page is under development..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using yfinance package to download the stock data from yahoo finance. Then I will try predicting the prices for each stock. My goal is to make a prediction on the expected value and variance of the price based on the previous values. Then I'll try to compute the experimental covarience between different stocks and input them to an optimization problem designed to reduce the mixed variance while maximizing the expected output. This is based on the mean-volatility predicate in financial analysis.  \n",
    "My initial intention was to use the rather new *tensorflow probability* capability to estimate the mean a variance for each output point. After some trials and failings, I have decided to use a simpler approach:\n",
    " - Use a small Nueral Net to compute a predicted value for the next days opening price for each stock.\n",
    " - Assume that my predictor assumes a normal distribution around the observed values (I should check this!). Therefore I can use the unbiased estimator of SD to find the expected variance around my predicted price.\n",
    " - Compute the correlations between different stocks experimentally (i.e. pairwise correlation between data columns)\n",
    " - perform the optimization as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import single_stock_predictor\n",
    "importlib.reload(single_stock_predictor)\n",
    "import pickle\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "#import mdn\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running for first time, you need to downloaded the stock symbols or \"tickers\". This is in done by setting parameter  \"get_tickers\".<br> Currently I'm downloading the daily data for 9years. If you already have downloaded some part of the data you can download the rest and append them to each other. Later I hope to automatize this section, since my goal is to run this script once weekly or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tickers = False \n",
    "read_tickers = False\n",
    "get_histories = False\n",
    "get_updated_data = False\n",
    "read_data = False\n",
    "read_updated_data = True\n",
    "get_business_info = True #you can go through business info field to exlude companies you don't want to invest in\n",
    "if get_updated_data or get_business_info:\n",
    "    read_tickers = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_tickers():\n",
    "    !curl -o /Users/abnousa/software/smartop/nasdaqtraded_companylist.txt ftp://ftp.nasdaqtrader.com/symboldirectory/nasdaqtraded.txt\n",
    "    symbols = pd.read_csv(\"/Users/abnousa/software/smartop/nasdaqtraded_companylist.txt\", sep = \"|\")\n",
    "    symbols = symbols.iloc[0:(symbols.shape[0] - 1),:] #last row is time\n",
    "    tickers = {}\n",
    "    failed = []\n",
    "    for i in symbols.index:\n",
    "        sym = symbols.iloc[i]['Symbol']\n",
    "        ticker = yf.Ticker(sym)\n",
    "        try:\n",
    "            check = ticker.calendar\n",
    "        except Exception as e:\n",
    "            print(' '.join([\"disregarding\", sym, type(e).__name__]))\n",
    "            failed.append(sym)\n",
    "            continue\n",
    "        print(' '.join([sym, 'added']))\n",
    "        name = symbols.iloc[i]['Security Name']\n",
    "        tickers[sym] = {'name': name, 'ticker': ticker}\n",
    "    sym_data = {'tickers':tickers, 'failed':failed}\n",
    "    with open('sym_data.pkl', 'wb') as symfile:\n",
    "        pickle.dump(sym_data, symfile)\n",
    "    return(sym_data)\n",
    "if get_tickers:\n",
    "    sym_data = download_tickers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_tickers:\n",
    "    with open('sym_data.pkl', 'rb') as symfile:\n",
    "        sym_data = pickle.load(symfile)\n",
    "        tickers, failed = sym_data['tickers'], sym_data['failed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_business_info(tickers):\n",
    "    for ticker in tickers.keys():\n",
    "        #print(ticker)\n",
    "        tickers[ticker]['business_summary'] = tickers[ticker]['ticker'].info.get('longBusinessSummary', None)\n",
    "    with open('sym_data.pkl', 'wb') as symfile:\n",
    "        pickle.dump(sym_data, symfile)\n",
    "if get_business_info:\n",
    "    download_business_info(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_histories(tickers):\n",
    "    period = \"9y\"\n",
    "    d = download_ticker_histories(tickers, period = period, interval = \"1d\", columns = ['Open'])\n",
    "    with open(\"daily_history_9y.pkl\", 'wb') as histfile:\n",
    "        pickle.dump(d, histfile)\n",
    "    return d\n",
    "if get_histories:\n",
    "    d = download_histories(sym_data['tickers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ticker_histories(tickers, start = None, period = None, end = None, interval = \"1d\", columns = ['Open']):\n",
    "    msft = yf.Ticker(\"MSFT\")\n",
    "    if start is None:\n",
    "        temp_hist = msft.history(period=\"9y\", interval=\"1d\")\n",
    "        end = list(temp_hist.index)[-1]\n",
    "        start = list(temp_hist.index)[0]\n",
    "    else:\n",
    "        temp_hist = msft.history(start = start, interval = \"1d\")\n",
    "        end = list(temp_hist.index)[-1]\n",
    "    d = pd.DataFrame(data = 0, columns = list(tickers.keys()), index = temp_hist.index)\n",
    "    for counter, i in enumerate(d.columns):\n",
    "        if counter % 100 == 0:\n",
    "            print(i)\n",
    "        hist = (tickers[i]['ticker'].history(start = start, end = end, interval = interval)[columns]).drop_duplicates(keep = 'last')\n",
    "        d[i] = hist\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_data:\n",
    "    with open(\"daily_history_9y.pkl\", 'rb') as infile:\n",
    "        d = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data(d, tickers, columns = ['Open']):\n",
    "    last_date = (pd.to_datetime(d.index.values[-1])).strftime(\"%Y-%m-%d\")\n",
    "    d_update = download_ticker_histories(tickers, start = last_date, interval = \"1d\", columns = columns)\n",
    "    d_update = d_update.iloc[1:,:]\n",
    "    d_update.head()\n",
    "    d_merged = pd.concat([d, d_update], axis = 0)\n",
    "    with open(\"daily_history_updated.pkl\", 'wb') as histfile:\n",
    "        pickle.dump(d_merged, histfile)\n",
    "    return d_merged\n",
    "if get_updated_data:\n",
    "    d = update_data(d, tickers, columns = ['Open'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_updated_data:\n",
    "    with open(\"daily_history_updated.pkl\", 'rb') as infile:\n",
    "        d = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(d.isna().sum(), bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.iloc[:, list(np.where(d.isna().sum() < 20)[0])]\n",
    "d = d.fillna(method = \"bfill\")\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import single_stock_predictor\n",
    "importlib.reload(single_stock_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = \"MSFT\"\n",
    "weekly = False\n",
    "window_size = 30\n",
    "batch_size = 32\n",
    "shuffle_buffer = None\n",
    "distributional = False\n",
    "epochs = 40\n",
    "training_points = 1500\n",
    "sd_estimate_required = True\n",
    "model_outdir = \"models\"\n",
    "training_verbosity = 1\n",
    "look_ahead_window = 5\n",
    "pred, train_sd, train_forecasts, valid_sd, valid_forecasts = single_stock_predictor.predict_tomorrow(stock_name, d, model_outdir = model_outdir, weekly = weekly, training_points = training_points, window_size = window_size, batch_size = batch_size, distributional = distributional, epochs = epochs, sd_estimate_required = sd_estimate_required, shuffle_buffer = shuffle_buffer, training_verbosity = training_verbosity, look_ahead_window = look_ahead_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "single_stock_predictor.plot_predictions(d[stock_name], train_forecasts, train_sd, window_size, limit_begin = 0, limit_end = 200, look_ahead_window = look_ahead_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "single_stock_predictor.plot_predictions(d[stock_name], train_forecasts, valid_sd, window_size, limit_begin = 0, limit_end = 200, look_ahead_window = look_ahead_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "single_stock_predictor.plot_predictions(d[stock_name][training_points:], valid_forecasts, train_sd, window_size, limit_begin = 0, limit_end = 200, look_ahead_window = look_ahead_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "single_stock_predictor.plot_predictions(d[stock_name][training_points:], valid_forecasts, valid_sd, window_size, limit_begin = 0, limit_end = 200, look_ahead_window = look_ahead_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_sd, valid_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay, so far we have showed that for the specified stock (here, MSFT aka microsoft) our model generates rather dependable predictions of price and our estimated standard deviation seems to be fitting at least visually. Of course, one can argue that we have picked an easy ticker, you'd expect microsoft to have a stable price. Well, I can't argue against that. But now, I'm going to randomly pick 10 tickers and perform the same operation on each of them. Before that I\n",
    "m going to use the *cov()* function from pandas to compute pairwise correlation between the selected tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "ticker_set = random.sample(list(d.columns), 10)\n",
    "ticker_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_select = d[ticker_set]\n",
    "print(d_select.shape)\n",
    "corels = d_select.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariances = d_select.cov()\n",
    "covariances.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corels_matrix = np.array(corels)\n",
    "heatmap(corels_matrix, corels.columns.values, corels.columns.values, cbarlabel = \"correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = False\n",
    "window_size = 30\n",
    "batch_size = 32\n",
    "shuffle_buffer = None\n",
    "distributional = False\n",
    "epochs = 20\n",
    "training_points = 1500\n",
    "sd_estimate_required = True\n",
    "model_outdir = \"models\"\n",
    "training_verbosity = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dict = {}\n",
    "for stock_name in ticker_set:\n",
    "    print(\" \".join([\"processing\", stock_name]))\n",
    "    pred, train_sd, train_forecasts, valid_sd, valid_forecasts = single_stock_predictor.predict_tomorrow(stock_name, d, model_outdir = model_outdir, weekly = weekly, training_points = training_points, window_size = window_size, batch_size = batch_size, distributional = distributional, epochs = epochs, sd_estimate_required = sd_estimate_required, shuffle_buffer = shuffle_buffer, training_verbosity = training_verbosity, look_ahead_window = look_ahead_window)\n",
    "    prediction_dict[stock_name] = {'expected_tomorrow': pred,\n",
    "                                   'train_sd': train_sd,\n",
    "                                   'train_forecasts': train_forecasts,\n",
    "                                   'valid_sd': valid_sd,\n",
    "                                   'valid_forecasts': valid_forecasts}\n",
    "    current_price = d.iloc[d.shape[0]-1][stock_name]\n",
    "    return_rate = pred / current_price - 1\n",
    "    prediction_dict[stock_name]['return_rate'] = return_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like for a few of the stocks, validation error was smaller than the training error. I'm curious why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here are the plots from validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(5,2,1)\n",
    "for plt_index, stock_name in enumerate(ticker_set):\n",
    "    ax = plt.subplot(5,2,plt_index + 1)\n",
    "    single_stock_predictor.plot_predictions(d[stock_name][training_points:], prediction_dict[stock_name]['valid_forecasts'], prediction_dict[stock_name]['valid_sd'], window_size, limit_begin = 0, limit_end = 200, ax = ax, legend = False, look_ahead_window = look_ahead_window)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and here are plots for training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(5,2,1)\n",
    "for plt_index, stock_name in enumerate(ticker_set):\n",
    "    ax = plt.subplot(5,2,plt_index + 1)\n",
    "    single_stock_predictor.plot_predictions(d[stock_name], prediction_dict[stock_name]['train_forecasts'], prediction_dict[stock_name]['valid_sd'], window_size, limit_begin = 0, limit_end = 200, ax = ax, legend = False, look_ahead_window = look_ahead_window)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_select.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock_name in ticker_set:\n",
    "    pred = prediction_dict[stock_name]['expected_tomorrow']\n",
    "    current_price = d.iloc[d.shape[0] - 1, :][stock_name]\n",
    "    return_rate = pred / current_price - 1\n",
    "    prediction_dict[stock_name]['return_rate'] = return_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_risks = pd.DataFrame({'return_rate':[prediction_dict[stock_name]['return_rate'] for stock_name in ticker_set],\n",
    "                        'risk' : [prediction_dict[stock_name]['valid_sd'] for stock_name in ticker_set],\n",
    "                         'price': d.iloc[d.shape[0] - 1][ticker_set]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows I'll be minimzing the mixed variance by selecting the \"best\" portfolio for a given return rate and budjet. For the optimization part I have heavily been dependent on the codes in [this tutorial](https://towardsdatascience.com/efficient-frontier-optimize-portfolio-with-scipy-57456428323e). Kudos to **J Li**. I have modified the code to compute the portfolio return rate and risk from mixture of normals distribution. Later I will also modify the optimization function by adding new constraints to compute the number of shares to buy with a given budget rather than the weight of instruments in the portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(rat_risks['risk'], rat_risks['return_rate'])\n",
    "for stock_name in ticker_set:\n",
    "    ax.annotate(stock_name, (prediction_dict[stock_name]['valid_sd'], prediction_dict[stock_name]['return_rate']))\n",
    "plt.xlabel(\"risk\")\n",
    "plt.ylabel(\"return rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variance of a linear combination of random variable can be computed by the formula below: [(source)](https://en.wikipedia.org/wiki/Variance)\n",
    "<img src=\"ext/lincomb_variance.png\" style=\"height:200px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_portfolio_risk(weights, rat_risks, covariances):\n",
    "    weight_matrix = np.outer(weights , weights)\n",
    "    weight_cov_combined = covariances * weight_matrix\n",
    "    mixed_var = np.sum(np.sum(weight_cov_combined))\n",
    "    return mixed_var\n",
    "\n",
    "def get_portfolio_return(weights, rat_risks):\n",
    "    total_return_rate = np.sum(rat_risks['return_rate'] * weights)\n",
    "    return total_return_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_weights(rat_risks, target_return=0.1):\n",
    "    instruments_count = rat_risks.shape[0]\n",
    "    init_guess = np.ones(instruments_count) * (1.0 / instruments_count)\n",
    "    bounds = ((0.0, 1.0),) * instruments_count\n",
    "    weights = minimize(get_portfolio_risk, init_guess,\n",
    "                       args=(rat_risks, covariances), method='SLSQP',\n",
    "                       options={'disp': False},\n",
    "                       constraints=({'type': 'eq', 'fun': lambda inputs: 1.0 - np.sum(inputs)},\n",
    "                                    {'type': 'eq', 'args': (rat_risks,),\n",
    "                                     'fun': lambda inputs, rat_risks:\n",
    "                                     target_return - get_portfolio_return(weights=inputs,\n",
    "                                                                          rat_risks=rat_risks)}\n",
    "                                   ),\n",
    "                       bounds=bounds)\n",
    "    return weights.x, weights.success, weights.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, success, status = optimize_weights(rat_risks)\n",
    "print(get_portfolio_risk(results, rat_risks, covariances))\n",
    "print(success)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(rat_risks['risk'], rat_risks['return_rate'])\n",
    "for i, stock_name in enumerate(ticker_set):\n",
    "    ax.annotate(' '.join([stock_name, str(round(weights[i], 5))]), (prediction_dict[stock_name]['valid_sd'], prediction_dict[stock_name]['return_rate']))\n",
    "plt.title(\"weigth of each instrument for minimized risk for 0.1 return rate\")\n",
    "plt.xlabel(\"risk\")\n",
    "plt.ylabel(\"rate of return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights seem to make sense, with the instruments with higher risk getting a weight of zero and the ones with higher return rate and small risk getting the largest of weights. \n",
    "Now let's modify the optimization function to accept and budget constraint as well as output number of shares per instrument (integer) rather than weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_portfolio_risk_by_shares(shares, rat_risks, covariances):\n",
    "    weights = shares / np.sum(shares)\n",
    "    weight_matrix = np.outer(weights , weights)\n",
    "    weight_cov_combined = covariances * weight_matrix\n",
    "    mixed_var = np.sum(np.sum(weight_cov_combined))\n",
    "    return mixed_var\n",
    "\n",
    "def get_portfolio_return_by_shares(shares, rat_risks, budget):\n",
    "    spent = np.sum(rat_risks['price'] * shares)\n",
    "    unspent = budget - spent\n",
    "    returns = spent * np.sum(rat_risks['return_rate'] * shares) + unspent\n",
    "    total_return_rate = (returns/budget) - 1\n",
    "    return total_return_rate\n",
    "\n",
    "def budget_constraint(shares, rat_risks, budget):\n",
    "    prices = np.array(rat_risks['price'])\n",
    "    unspent = budget - np.sum(shares * prices)\n",
    "    return(unspent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_shares(rat_risks, target_return=0.1, budget = 2000):\n",
    "    #normalized_prices = prices / prices.ix[0, :]\n",
    "    instruments_count = rat_risks.shape[0]\n",
    "    init_guess = np.ones(instruments_count) * 2 #(1.0 / instruments_count)\n",
    "    bounds = ((0.0, np.inf),) * instruments_count\n",
    "    shares = minimize(get_portfolio_risk_by_shares, init_guess,\n",
    "                       args=(rat_risks, covariances), method='SLSQP',\n",
    "                       options={'disp': False},\n",
    "                       constraints=({'type': 'ineq', 'fun': lambda x: budget_constraint(x, rat_risks, budget)}, #make sure total is less than budget\n",
    "                                    {'type': 'eq', 'args': (rat_risks, budget), #make the return rate equal to the expected rate\n",
    "                                     'fun': lambda inputs, rat_risks, budget:\n",
    "                                     target_return - get_portfolio_return_by_shares(inputs, rat_risks, budget)},\n",
    "                                    {'type':'eq','fun': lambda x : max([0] + [x[i]-int(x[i]) for i in range(len(x)) if x[i]-int(x[i]) > 0])}, #try to make them as close to integer as possible\n",
    "                                   ),\n",
    "                       bounds=bounds\n",
    "                     )\n",
    "    return shares.x, shares.success, shares.status, shares.message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 2000\n",
    "share_distribution, success, status, message = optimize_shares(rat_risks, budget = budget)\n",
    "share_distribution\n",
    "output = rat_risks\n",
    "output['shares'] = np.floor(share_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(['retrun rate', str(get_portfolio_return_by_shares(rat_risks['shares'], rat_risks, budget))]))\n",
    "print(' '.join(['risk', str(get_portfolio_risk_by_shares(rat_risks['shares'], rat_risks, covariances))]))\n",
    "print(' '.join(['invested:', str(np.sum(rat_risks['shares'] * rat_risks['price']))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t = pd.DataFrame({'weights':weights})\n",
    "weights = np.random.randint(1, 10, size = 4)\n",
    "print(weights)\n",
    "weight_mat = np.outer(weights , weights)\n",
    "#pd.pivot_table(t, values = ['weights'], index = 'weights', columns = ['weights'])\n",
    "#t.info()\n",
    "print(weight_mat)\n",
    "#np.stack(weights, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_mat = np.outer(weights , weights)\n",
    "mv = covariances.iloc[-4:,-4:] * weight_mat\n",
    "np.sum(np.sum(mv))\n",
    "print(mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.sum(mv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(covariances.iloc[-4:,-4:])\n",
    "print(weight_mat)\n",
    "print(covariances.iloc[-4:,-4:] * weight_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-11.118789 * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funnction copied from matplotlib gallery\n",
    "def heatmap(data, row_labels, col_labels, ax=None,\n",
    "            cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a numpy array and two lists of labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (N, M).\n",
    "    row_labels\n",
    "        A list or array of length N with the labels for the rows.\n",
    "    col_labels\n",
    "        A list or array of length M with the labels for the columns.\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to `imshow`.\n",
    "    \"\"\"\n",
    "\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    # ... and label them with the respective list entries.\n",
    "    ax.set_xticklabels(col_labels)\n",
    "    ax.set_yticklabels(row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    for edge, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im, cbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d.index.values)[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_risks.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the stock data have a lot of missing points over the past 9 years. This might be because they were founded later or had their IPO sometime during this time-period. Below I take a look at the number of stocks with various numbers of missing points and filter the data to the ones with less than 20 NA's. I will impute the missing points by \"backward filling\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now we have the data and for each stock we can take a look at their trend. Change the *ticker_name* below and have a look at the plot. I'm plotting the Microsoft stock prices from some day in December 2012 up to December 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_name = \"MSFT\"\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(d[ticker_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is copied exactly from the deeplearning.ai course on time-series analysis on coursera!\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer, shuffle = True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    #dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (tf.expand_dims(window[:-1], axis = -1), window[-1]))\n",
    "        #dataset = dataset.map(lambda window: (window[0].reshape((len(window[0], 1))), window[1]))\n",
    "    else:\n",
    "        dataset = dataset.map(lambda window: (tf.expand_dims(window[:-1], axis = -1), window[-1]))\n",
    "        #dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = \"MSFT\"\n",
    "weekly = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = d.iloc[:, :1]#:int(d.shape[1]/2)]\n",
    "d = d.loc[:,[stock_name]]\n",
    "d.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weekly:\n",
    "    d.reset_index(drop = False, inplace = True)\n",
    "    d['weekday'] = d['Date'].dt.day_name()\n",
    "    d = d[d['weekday'] == \"Friday\"]\n",
    "    d.drop(['Date', 'weekday'], axis = 1, inplace = True)\n",
    "    print(d.shape)\n",
    "    print(d.head())\n",
    "    d.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_risks.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = d.reset_index(drop = True)\n",
    "plt.plot(p[stock_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 30\n",
    "batch_size = 32\n",
    "shuffle_buffer = 100000\n",
    "training_points = 350 if weekly else 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = d / train_d.max(axis = 0)\n",
    "train_d = d.iloc[:training_points, :]\n",
    "valid_d = d.iloc[training_points:, :]\n",
    "train_df = windowed_dataset(train_d.to_numpy().reshape((len(train_d,))), window_size, batch_size, shuffle_buffer)\n",
    "normalization_factors = train_d.max()\n",
    "valid_df = windowed_dataset(valid_d.to_numpy().reshape((len(valid_d,))), window_size, batch_size, shuffle_buffer, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df\n",
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#td = windowed_dataset(np.array(train_d['MSFT']), window_size, batch_size, shuffle_buffer)\n",
    "#td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_y = train_d[30:]\n",
    "#train_y.shape\n",
    "train_d[stock_name].shape\n",
    "train_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d.describe()\n",
    "valid_d.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_normalize(x, factors = None, denorm = False, input_mean = None, input_std = None):\n",
    "    if factors is None:\n",
    "        factors = tf.reduce_max(x, axis = 0, keepdims = False)\n",
    "        #tf.print(factors)\n",
    "        tf.print(\"pre mod shape of x\")\n",
    "        tf.print(tf.shape(x))\n",
    "        tf.print(\"shape of factors\")\n",
    "        tf.print(tf.shape(factors))\n",
    "    input_mean = tf.math.reduce_mean(x, axis = 0)\n",
    "    input_std = tf.keras.backend.std(x)\n",
    "    if denorm:\n",
    "        #x += 1\n",
    "        factors = 1. / factors\n",
    "    x /= factors\n",
    "    '''\n",
    "    if not denorm:\n",
    "        if len(x.get_shape()) == 3:\n",
    "            #tf.print('mean', input_mean)\n",
    "            #tf.print('std')\n",
    "            #tf.print('std', input_std)\n",
    "            #tf.print('oldx')\n",
    "            #tf.print('shape',x.get_shape())\n",
    "            #tf.print(x)\n",
    "            x = (x - input_mean)/input_std\n",
    "    else:\n",
    "        #tf.print('mean')       \n",
    "        #tf.print(len(x.get_shape()))\n",
    "        x = x * input_std + input_mean\n",
    "    '''\n",
    "    #x = (x - tf.keras.backend.mean(x)) / tf.keras.backend.std(x)\n",
    "    #tf.print(\"post mod shape of x\")\n",
    "    #tf.print(tf.shape(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributional = False\n",
    "tf.compat.v1.reset_default_graph()\n",
    "#output_size = train_d.shape[1]\n",
    "tfd = tfp.distributions\n",
    "output_dense_size = 2 if distributional else 1\n",
    "\n",
    "def activations(l, input_mean = None, input_std = None, window_size = None):\n",
    "    l_0 = (tf.keras.activations.linear(l[...,0])) * input_mean #* input_std) + input_mean #* normalization_factors\n",
    "    #l_1 = std_multiplier + \n",
    "    l_1 = ((tf.keras.activations.linear(tf.abs(l[...,1])))) + 1e-6 # * input_std + input_mean) / window_size) / 0.5\n",
    "    #tf.print(l_1)\n",
    "    lnew = tf.stack([l_0, l_1], axis = 1)\n",
    "    return lnew\n",
    "\n",
    "def simple_activations(l):\n",
    "    l_0 = tf.keras.activations.linear(l[...,0])\n",
    "    l_1 = tf.keras.activations.elu(l[...,1])\n",
    "    lnew = tf.stack([l_0, l_1], axis = 1)\n",
    "    return lnew\n",
    "\n",
    "initializers = \"glorot_normal\"\n",
    "activation_name = 'relu'\n",
    "model = tf.keras.models.Sequential([\n",
    "  #tf.keras.layers.LayerNormalization(axis = 0),\n",
    "  #tf.keras.layers.Lambda(layer_normalize, arguments={'factors': normalization_factors, 'input_mean' : np.mean(train_d)[0], 'input_std': np.std(train_d)[0], 'denorm' : False}, input_shape = (window_size,)),\n",
    "  #tf.keras.layers.GRU(32, return_sequences = True, kernel_initializer = initializers, activation = activation_name, input_shape = (window_size, 1)), \n",
    "  #tf.keras.layers.Conv1D(128, kernel_size = 3),\n",
    "  #tf.keras.layers.AveragePooling1D(pool_size = 3, padding = 'valid'),\n",
    "  #tf.keras.layers.LSTM(64, return_sequences=True, kernel_initializer = initializers, activation = activation_name),\n",
    "  #tf.keras.layers.LSTM(256, return_sequences=True, kernel_initializer = initializers),\n",
    "  #tf.keras.layers.Dropout(0.5),\n",
    "  #tf.keras.layers.LSTM(64, return_sequences=True, kernel_initializer = initializers, activation = activation_name),\n",
    "  #tf.keras.layers.LSTM(128, return_sequences=True, kernel_initializer = initializers),\n",
    "  #tf.keras.layers.Dropout(0.1),\n",
    "  #tf.keras.layers.GRU(8, return_sequences=True, kernel_initializer = initializers),\n",
    "  tf.keras.layers.GRU(64, kernel_initializer = initializers, activation = activation_name, input_shape = (window_size, 1)),\n",
    "  #tf.keras.layers.LSTM(32, kernel_initializer = initializers, activation = activation_name),\n",
    "  #tf.keras.layers.Dense(output_dense_size, activation = tf.keras.layers.Activation(lambda x: activations(x, np.mean(train_d)[0], np.std(train_d)[0], window_size)), kernel_initializer = \"he_normal\"),\n",
    "  #tf.keras.layers.Lambda(layer_normalize, arguments={'factors': normalization_factors, 'denorm' : True}),\n",
    "  #tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=tf.abs(t[..., 0]), scale=0.01*(tf.abs(t[..., 1]))))#-t[...,0]))))#t[...,1])) \n",
    "  #                         #scale=(tf.keras.backend.std[...,1])))\n",
    "  #tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[:,0], scale = t[...,0] + tf.keras.backend.std(t[:,1])))\n",
    "  #                         #scale=1e-3 + tf.math.softplus(0.05 * t[..., 1:]))),\n",
    "  #tfp.layers.IndependentNormal(output_size)\n",
    "  #tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t, scale=1))\n",
    "])\n",
    "\n",
    "if distributional:\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        output_dense_size, activation = 'linear',\n",
    "        #activation = tf.keras.layers.Activation(lambda x: activations(x, np.mean(train_d)[0], np.std(train_d)[0], window_size)),\n",
    "        #activation = tf.keras.layers.Activation(lambda x: simple_activations(x)),\n",
    "        kernel_initializer = \"he_uniform\"))\n",
    "    model.add(tfp.layers.DistributionLambda(lambda t: \n",
    "                                            tfd.Normal(\n",
    "                                                loc=tf.abs(t[..., 0]), \n",
    "                                                scale= 1e-6 + tf.abs(t[..., 1]) #* normalization_factors\n",
    "                                            )))\n",
    "else: \n",
    "    #model.add(tf.keras.layers.Dense(128, activation = 'linear'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation = 'linear', kernel_initializer = initializers))\n",
    "    #model.add(tf.keras.layers.Lambda(layer_normalize, arguments={'factors': normalization_factors, 'input_mean' : np.mean(train_d)[0], 'input_std': np.std(train_d)[0], 'denorm' : True}))\n",
    "#model.add(tf.keras.layers.Lambda(layer_normalize, arguments={'factors': normalization_factors, 'denorm' : True}))\n",
    "    \n",
    "negloglik = lambda y, p_y: -p_y.log_prob(y)\n",
    "#negcheck = lambda y, yh: tf.math.abs(y - yh[:,0])\n",
    "#optimizer = tf.keras.optimizers.Adam()#.minimize(cost)\n",
    "#model.compile(#loss=tf.keras.losses.Huber(),\n",
    "#              loss=negloglik,\n",
    "#              #loss = 'mse',\n",
    "#              #loss = cost,\n",
    "#              optimizer=optimizer,\n",
    "#              #optimizer = 'Adam',\n",
    "#              metrics = ['mae']\n",
    "#             )\n",
    "#model.build(input_shape = (batch_size, window_size, None))\n",
    "#model.summary()\n",
    "#history = model.fit(df, epochs=2, callbacks = [lr_schedule])\n",
    "#history = model.fit(train_df, epochs=10, validation_data = valid_df)\n",
    "\n",
    "loss_function = negloglik if distributional else 'mse' #tf.keras.losses.Huber()\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=loss_function)\n",
    "model.fit(train_df, epochs=20, verbose=True, validation_data = valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xh = model.predict(train_df)\n",
    "forecasts_x = []\n",
    "sds = []\n",
    "#indices = [j for j in range(x.get_shape()[0])]      \n",
    "#print(indices)\n",
    "vs = np.array(train_d)\n",
    "#xs = list(x[i,:,:] for i in indices)\n",
    "for time in range(len(train_d) - window_size):\n",
    "    prediction = model.predict(vs[time:time+window_size].reshape((1, window_size, 1)))\n",
    "    forecasts_x.append(prediction)\n",
    "    sds.append(prediction - vs[time+window_size])\n",
    "sd_estimate = np.sqrt(np.sum(np.array(sds)**2) / (training_points - window_size - 1))\n",
    "forecasts_x = np.array(forecasts_x)[:,0].reshape((training_points - window_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_begin = 150\n",
    "limit_end = 300\n",
    "plt.plot(np.array(train_d[(window_size + limit_begin): (window_size + limit_end)]), '.')\n",
    "plt.plot(forecasts_x[limit_begin:limit_end], '.')\n",
    "plt.plot(forecasts_x[limit_begin:limit_end] + 2*sd_estimate, '-')\n",
    "plt.plot(forecasts_x[limit_begin:limit_end] - 2*sd_estimate, '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomorrows_prediction = model.predict(np.array(d[stock_name])[-window_size:].reshape(1, window_size, 1))[0, 0]\n",
    "tomorrows_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_points - window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xh[:10])\n",
    "print(train_d[window_size:window_size + 10])\n",
    "print((xh - np.array(train_d[window_size:]))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = []\n",
    "#sds = []\n",
    "#indices = [j for j in range(x.get_shape()[0])]      \n",
    "#print(indices)\n",
    "vs = np.array(valid_d)\n",
    "#xs = list(x[i,:,:] for i in indices)\n",
    "for time in range(len(valid_d) - window_size):\n",
    "    prediction = model.predict(vs[time:time+window_size].reshape((1, window_size, 1)))\n",
    "    forecasts.append(prediction)\n",
    "    sds.append(vs[time+window_size] - prediction)\n",
    "    #forecasts.append(model(vs[time:time+window_size].reshape((1, window_size, 1))))\n",
    "#print(vs[:window_size])\n",
    "#print(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(forecasts))\n",
    "forecasts[1]\n",
    "if distributional:\n",
    "    ymeans = np.array([i.mean() for i in forecasts])\n",
    "    ystdv = np.array([i.stddev() for i in forecasts])\n",
    "else:\n",
    "    forecasts = np.array(forecasts[:,0,0])\n",
    "    sds = np.array(sds)[:,0,0]\n",
    "forecasts\n",
    "sd_estimate = np.sqrt(np.sum(sds ** 2)/(sds.shape[0]-1))\n",
    "sd_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts[0].shape\n",
    "results = np.array(forecasts)[:, 0]\n",
    "np.max(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(valid_d)[window_size + 100: window_size + 150], '.', label = 'obs')\n",
    "plt.plot(forecasts[100:150], '.', label = 'pred')\n",
    "plt.plot(forecasts[100:150] + sd_estimate, '-')\n",
    "plt.plot(forecasts[100:150] - sd_estimate, '-')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## draftpad: (nothing interesting below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(shape = (batch_size, window_size))\n",
    "y = np.zeros(shape = (batch_size, 1))\n",
    "for i, j in valid_df:\n",
    "    x = i\n",
    "    y = j\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape, y.shape, distributional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (distributional):\n",
    "    ys = model(x)\n",
    "    yhat = ys.mean()\n",
    "    ydev = ys.stddev()\n",
    "else:\n",
    "    #ys = pd.DataFrame(model.predict(x))\n",
    "    ys = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y, '.', label = 'obs')\n",
    "if distributional:\n",
    "    plt.plot(yhat, 'r.', label = 'pred')\n",
    "    plt.plot(yhat + 2*ydev, '-')\n",
    "    plt.plot(yhat - 2*ydev, '-')\n",
    "else:\n",
    "    plt.plot(ys[:50, 0], '.', label = 'pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = vs[:30].reshape((1, 30))\n",
    "ts.shape\n",
    "model.predict(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs[time:time+window_size].reshape((window_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y, 'b.')\n",
    "plt.plot(model(x)[:,0], 'r.')\n",
    "plt.plot(model(x)[:,0] + model(x)[:,1], '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(valid_df)\n",
    "#p = p.reshape((1480, 4165))\n",
    "print(d.shape)\n",
    "print(p.shape)\n",
    "print(valid_d.describe())\n",
    "p = pd.DataFrame(p) #* d.max(axis = 0).reset_index(drop = True)\n",
    "print(p.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = yhat.mean()\n",
    "stddev = yhat.std()\n",
    "mean_plus_2_stddev = mean - 2. * stddev\n",
    "mean_minus_2_stddev = mean + 2. * stddev\n",
    "#plt.plot(mean)#, 'o')\n",
    "plt.plot(yhat, '.')\n",
    "plt.plot(vd*normalization_factors + mean_plus_2_stddev, '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd = valid_d.reset_index(drop = True)\n",
    "normalization_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_cost(mu, sigma, y):\n",
    "    dist = tf.distributions.Normal(loc=mu, scale=sigma)\n",
    "    return tf.reduce_mean(-dist.log_prob(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I'm trying to output mean and variance for each stock, I will need two units in my output layer and each will most likely require a different activation function. This can be done [at least more neatly] in funnctional API of keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = train_d.shape[1]\n",
    "variables = train_d.shape[1]\n",
    "#output_size = 1\n",
    "print(output_size)\n",
    "inputs = tf.keras.Input(shape=(None, variables))\n",
    "normalization_layer = tf.keras.layers.Lambda(layer_normalize, arguments={'factors': normalization_factors, 'denorm' : False})(inputs)\n",
    "output_1 = tf.keras.layers.LSTM(128, return_sequences=True)(normalization_layer)\n",
    "output_2 = tf.keras.layers.LSTM(256, return_sequences=True)(output_1)\n",
    "output_3 = tf.keras.layers.LSTM(256, return_sequences=True)(output_2)\n",
    "output_4 = tf.keras.layers.LSTM(128, return_sequences=False)(output_3)\n",
    "#predictions = tf.keras.layers.Dense(1, activation='elu')(output_4)\n",
    "\n",
    "#sigmoid_out = tf.keras.layers.Dense(units=1, activation=tf.nn.sigmoid)\n",
    "#relu_out = tf.keras.layers.Dense(units=1, activation=tf.nn.relu)\n",
    "#out = tf.concat([sigmoid_out, relu_out], axis=1)\n",
    "\n",
    "#denorm_layer = tf.keras.layers.Lambda(layer_normalize, arguments={'factors': normalization_factors, 'denorm' : True})(predictions)\n",
    "\n",
    "mu = tf.keras.layers.Dense(units=1, activation = 'elu')(output_4)\n",
    "sigma = tf.keras.layers.Dense(units=1,activation=lambda x: tf.nn.elu(x) + 1)(output_4)\n",
    "\n",
    "mu_denorm = tf.keras.layers.Lambda(layer_normalize, arguments={'factors': normalization_factors, 'denorm' : True})(mu)\n",
    "sigma_denorm = tf.keras.layers.Lambda(layer_normalize, arguments={'factors': normalization_factors, 'denorm' : True})(sigma)\n",
    "\n",
    "out = tf.concat([sigma_denorm, mu_denorm], axis=1)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "\n",
    "'''\n",
    "sigmoid_out = tf.keras.layers.Dense(units=1, activation=tf.nn.sigmoid)\n",
    "relu_out = tf.keras.layers.Dense(units=1, activation=tf.nn.relu)\n",
    "out = tf.concat([sigmoid_out, relu_out], axis=1)\n",
    "inner_layers.add(out)\n",
    "\n",
    "denorm_layer = tf.keras.layers.Lambda(layer_normalize, arguments={'factors': normalization_factors, 'denorm' : True})\n",
    "\n",
    "inner_layers.add(denorm_layer)\n",
    "model = inner_layers\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-2 * 10**(epoch / 20))\n",
    "#optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
    "#optimizer = tf.keras.optimizers.SGD(lr=1e-7, momentum=0.9)\n",
    "#optimizer = tf.keras.optimizers.SGD(lr=1e-10, momentum = 0.9, clipnorm = 2)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "#optimizer = tf.keras.optimizers.RMSprop()\n",
    "'''\n",
    "cost = mdn_cost(mu_denorm, sigma_denorm, y)\n",
    "optimizer = tf.keras.optimizers.Adam().minimize(cost)\n",
    "model.compile(#loss=tf.keras.losses.Huber(),\n",
    "              #loss = 'mse',\n",
    "              loss = cost,\n",
    "              optimizer=optimizer,\n",
    "              #optimizer = 'Adam',\n",
    "              metrics = ['mae']\n",
    "             )\n",
    "#model.build(input_shape = (batch_size, window_size, None))\n",
    "#model.summary()\n",
    "#history = model.fit(df, epochs=2, callbacks = [lr_schedule])\n",
    "history = model.fit(train_df, epochs=10, validation_data = valid_df)#, callbacks = [lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = train_d.shape[1]\n",
    "#output_size = 1\n",
    "print(output_size)\n",
    "model = tf.keras.models.Sequential([\n",
    "  #tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]), #if the input is one stock\n",
    "  tf.keras.layers.Lambda(layer_normalize, arguments={'factors': normalization_factors, 'denorm' : False}),\n",
    "  #tf.keras.layers.Conv1D(128, kernel_size = 3, input_shape = (batch_size, window_size, None)),\n",
    "  #tf.keras.layers.AveragePooling1D(),\n",
    "  #tf.keras.layers.Conv1D(256, kernel_size = 3),\n",
    "  #tf.keras.layers.AveragePooling1D(),\n",
    "  #tf.keras.layers.Conv1D(256, kernel_size = 1),\n",
    "  #tf.keras.layers.AveragePooling1D(),\n",
    "  #tf.keras.layers.Conv1D(256, kernel_size = 1, activation = 'relu'),\n",
    "  #tf.keras.layers.AveragePooling1D(),\n",
    "  #tf.keras.layers.GRU(512, return_sequences=True),\n",
    "  #tf.keras.layers.Dropout(0.5),\n",
    "  #tf.keras.layers.GRU(256, return_sequences=True),\n",
    "  #tf.keras.layers.GRU(256, return_sequences=True),\n",
    "  #tf.keras.layers.GRU(256, return_sequences=True),\n",
    "  #tf.keras.layers.GRU(256),\n",
    "  #tf.keras.layers.SimpleRNN(128, return_sequences=True),\n",
    "  #tf.keras.layers.SimpleRNN(256, return_sequences=True),\n",
    "  #tf.keras.layers.SimpleRNN(512, return_sequences=True),\n",
    "  #tf.keras.layers.SimpleRNN(256, return_sequences=True),\n",
    "  #tf.keras.layers.SimpleRNN(128),\n",
    "  tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "  tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "  #tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "  tf.keras.layers.LSTM(128),\n",
    "  #tf.keras.layers.Dropout(0.5),\n",
    "  #tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "  #tf.keras.layers.Dropout(0.5),\n",
    "  #tf.keras.layers.LSTM(256, return_sequences=True, activation = 'relu'),\n",
    "  #tf.keras.layers.Dropout(0.5),\n",
    "  #tf.keras.layers.LSTM(256, return_sequences=True, activation = 'relu'),\n",
    "  #tf.keras.layers.Dropout(0.5),\n",
    "  #tf.keras.layers.LSTM(256, return_sequences=True, activation = 'relu'),\n",
    "  #tf.keras.layers.Dropout(0.5),\n",
    "  #tf.keras.layers.LSTM(256, return_sequences=True, activation = 'relu'),\n",
    "  #tf.keras.layers.LSTM(256, return_sequences=True, activation = 'relu'),\n",
    "  #tf.keras.layers.LSTM(256, return_sequences=True, activation = 'relu'),\n",
    "  #tf.keras.layers.Dropout(0.5),\n",
    "  #tf.keras.layers.LSTM(256, return_sequences=True, activation = 'relu'),\n",
    "  #tf.keras.layers.LSTM(256, return_sequences=True, activation = 'relu'),\n",
    "  #tf.keras.layers.LSTM(256, activation = 'tanh', return_sequences=True),\n",
    "  #tf.keras.layers.LSTM(128),\n",
    "  #tf.keras.layers.SimpleRNN(64, return_sequences = True),\n",
    "  #tf.keras.layers.SimpleRNN(128, return_sequences = True),\n",
    "  #tf.keras.layers.SimpleRNN(256),\n",
    "  tf.keras.layers.Dense(output_size, activation = 'elu'),\n",
    "  #tf.keras.layers.Lambda(lambda x: x + 1),\n",
    "  tf.keras.layers.Lambda(layer_normalize, arguments={'factors': normalization_factors, 'denorm' : True})\n",
    "])\n",
    "\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-2 * 10**(epoch / 20))\n",
    "#optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
    "#optimizer = tf.keras.optimizers.SGD(lr=1e-7, momentum=0.9)\n",
    "#optimizer = tf.keras.optimizers.SGD(lr=1e-10, momentum = 0.9, clipnorm = 2)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "#optimizer = tf.keras.optimizers.RMSprop()\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              #loss = 'mse',\n",
    "              optimizer=optimizer,\n",
    "              #optimizer = 'Adam',\n",
    "              metrics = ['mae']\n",
    "             )\n",
    "#model.build(input_shape = (batch_size, window_size, None))\n",
    "#model.summary()\n",
    "#history = model.fit(df, epochs=2, callbacks = [lr_schedule])\n",
    "history = model.fit(train_df, epochs=10, validation_data = valid_df)#, callbacks = [lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.to_numpy()[50:55, :8]\n",
    "train_d.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p = model.predict(np.array(d['MSFT']).reshape((len(d['MSFT']), 1)))\n",
    "p = model.predict(train_df)\n",
    "#p = p.reshape((1480, 4165))\n",
    "print(d.shape)\n",
    "print(p.shape)\n",
    "p = pd.DataFrame(p)\n",
    "p2 = p #* d.max(axis = 0).reset_index(drop = True)\n",
    "p2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MDN MODEL\n",
    "\n",
    "N_MIXES = 1  # number of mixture components\n",
    "OUTPUT_DIMS = 1  # number of real-values predicted by each mixture component\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Lambda(layer_normalize, arguments={'factors': normalization_factors, 'denorm' : False}),\n",
    "  tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "  tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "  tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "  tf.keras.layers.LSTM(128),\n",
    "  tf.keras.layers.Dense(1, activation = 'elu'),\n",
    "  tf.keras.layers.Lambda(layer_normalize, arguments={'factors': normalization_factors, 'denorm' : True})\n",
    "])\n",
    "\n",
    "model.add(mdn.MDN(OUTPUT_DIMS, N_MIXES))\n",
    "model.compile(loss=mdn.get_mixture_loss_func(1,N_MIXES), optimizer=tf.keras.optimizers.Adam())\n",
    "#model(train_df)\n",
    "#model.summary()\n",
    "history = model.fit(train_df, epochs=10)\n",
    "p = model.predict(train_df)\n",
    "y_samples = np.apply_along_axis(mdn.sample_from_output, 1, p, OUTPUT_DIMS, N_MIXES, temp=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.DataFrame({'s' : y_samples[:, 0, 0] * normalization_factors[0]})\n",
    "s.describe()\n",
    "#normalization_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(valid_df)\n",
    "#p = p.reshape((1480, 4165))\n",
    "print(d.shape)\n",
    "print(p.shape)\n",
    "print(valid_d.describe())\n",
    "p = pd.DataFrame(p) #* d.max(axis = 0).reset_index(drop = True)\n",
    "print(p.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_d.describe())\n",
    "print(train_d.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_d.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = model.predict(df)\n",
    "pd.DataFrame(p2).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
    "plt.axis([1e-8, 1e-1, 130000, 170000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(d.columns.values == \"MSFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[:, 2572].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tickers[0].info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = yf.Tickers(' '.join(list(tickers.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = t.history(period=\"9y\", interval = '1d')\n",
    "?Tickers.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yf.Tickers.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.history(period=\"9y\", interval = '1d', 'Open')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install keras-mdn-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.iloc[d.shape[0] - 1][ticker_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft = yf.Ticker(\"MSFT\")\n",
    "temp_hist = msft.history(period=\"9y\", interval=\"1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld = d.index.values[-1]\n",
    "print(ld)\n",
    "p = pd.to_datetime([ld])\n",
    "p = p.strftime(\"%Y-%m-%d\")\n",
    "p.values[0]\n",
    "#pd.DatetimeIndex(np.datetime_as_string(ld))\n",
    "last_date = (pd.to_datetime(d.index.values[-1])).strftime(\"%Y-%m-%d\")\n",
    "print(last_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = msft.history(start = last_date, interval='1d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = tickers['BKJ']['ticker'].history(start = last_date, interval = \"1d\")[['Open']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = hist.drop_duplicates(keep = 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agilent Technologies, Inc. Common Stock\n",
      "Agilent Technologies, Inc. provides application focused solutions to the life sciences, diagnostics, and applied chemical markets worldwide. It operates in three segments: Life Sciences and Applied Markets, Diagnostics and Genomics, and Agilent CrossLab. The Life Sciences and Applied Markets segment offers liquid and gas chromatography systems and components; liquid and gas chromatography mass spectrometry systems; inductively coupled plasma mass spectrometry instruments; atomic absorption instruments; microwave plasma-atomic emission spectrometry instruments; inductively coupled plasma optical emission spectrometry instruments; raman spectroscopy; cell analysis plate based assays; flow cytometer; real-time cell analyzer; laboratory software and information management and analytics; laboratory automation and robotic systems; dissolution testing; vacuum pumps; and measurement technologies. The Diagnostics and Genomics segment provides reagents, instruments, software, and consumables; arrays for DNA mutation detection, genotyping, gene copy number determination, identification of gene rearrangements, DNA methylation profiling, and gene expression profiling, as well as sequencing target enrichment, genetic data management, and interpretation support software; and equipment focused on production of synthesized oligonucleotides for use as active pharmaceutical ingredients. The Agilent CrossLab segment offers GC and LC columns, sample preparation products, custom chemistries, and laboratory instrument supplies; and startup, operational, training, compliance support, and software as a service, as well as asset management and consultation services. The company markets its products through direct sales, distributors, resellers, manufacturer's representatives, and electronic commerce. Agilent Technologies, Inc. has collaboration agreement with University of Duisburg-Essen. The company was founded in 1999 and is headquartered in Santa Clara, California.\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "print(tickers[list(tickers.keys())[counter]]['name'])\n",
    "print(tickers[list(tickers.keys())[counter]]['business_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
